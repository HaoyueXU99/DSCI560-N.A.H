{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install PyPDF2\n","!pip install pytesseract\n","!pip install pdf2image\n","!apt-get install -y poppler-utils\n","!apt install tesseract-ocr\n","!pip install selenium"],"metadata":{"id":"WU6gV-WOoNvB","executionInfo":{"status":"ok","timestamp":1696720391017,"user_tz":420,"elapsed":39359,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0eed209-637a-4f84-c9fe-f0dcec636dd5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.10\n","Collecting pdf2image\n","  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n","Installing collected packages: pdf2image\n","Successfully installed pdf2image-1.16.3\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 696 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n","Fetched 186 kB in 1s (125 kB/s)\n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 120875 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  tesseract-ocr-eng tesseract-ocr-osd\n","The following NEW packages will be installed:\n","  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n","0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 4,816 kB of archives.\n","After this operation, 15.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n","Fetched 4,816 kB in 2s (1,946 kB/s)\n","Selecting previously unselected package tesseract-ocr-eng.\n","(Reading database ... 120905 files and directories currently installed.)\n","Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n","Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n","Selecting previously unselected package tesseract-ocr-osd.\n","Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n","Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n","Selecting previously unselected package tesseract-ocr.\n","Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n","Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n","Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n","Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n","Setting up tesseract-ocr (4.1.1-2.1build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting selenium\n","  Downloading selenium-4.13.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.6)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n","Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n","Successfully installed h11-0.14.0 outcome-1.2.0 selenium-4.13.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"id":"8H0MauikoCdL","executionInfo":{"status":"ok","timestamp":1696720391904,"user_tz":420,"elapsed":892,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"outputs":[],"source":["import os\n","import re\n","import glob\n","import time\n","import PyPDF2\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from collections import defaultdict\n","from selenium import webdriver\n","from pdf2image import convert_from_path\n","import pytesseract\n"]},{"cell_type":"code","source":["#HARD CODE STIMULATION PAGE NUMBER FOR EACH FILE\n","stimulation_page = {'W21796.pdf':19,'W22099.pdf':5,'W22220.pdf':15,'W22221.pdf':4,'W21266.pdf':19,'W20864.pdf':16,'W20863.pdf':15,'W20407.pdf':18,\n","                    'W20197.pdf':84,'W22247.pdf':47,'W22249.pdf':7,'W22731.pdf':8,'W22740.pdf':15,'W23230.pdf':5,'W23359.pdf':6,'W23360.pdf':5,\n","                    'W23361.pdf':6,'W23362.pdf':5,'W23363.pdf':6,'W23364.pdf':5,'W23365.pdf':6,'W23366.pdf':5,'W23367.pdf':9,'W23368.pdf':6,\n","                    'W23369.pdf':8,'W23370.pdf':5,'W23371.pdf':7,'W23372.pdf':6,'W25156.pdf':15,'W25157.pdf':9,'W25158.pdf':11,'W25159.pdf':8,\n","                    'W25160.pdf':9,'W25571.pdf':4,'W28190.pdf':4,'W28194.pdf':4,'W28303.pdf':6,'W28342.pdf':4,'W28394.pdf':6,'W28425.pdf':13,\n","                    'W28554.pdf':4,'W28557.pdf':11,'W28599.pdf':6,'W28600.pdf':7,'W28601.pdf':7,'W28633.pdf':7,'W28634.pdf':5,'W28636.pdf':5,\n","                    'W28648.pdf':6,'W28649.pdf':6,'W28651.pdf':5,'W28652.pdf':77,'W28654.pdf':8,'W28655.pdf':5,'W28658.pdf':3,'W28744.pdf':11,\n","                    'W28754.pdf':6,'W28755.pdf':6,'W28756.pdf':5,'W28976.pdf':6,'W28978.pdf':3,'W29242.pdf':5,'W29244.pdf':5,'W29316.pdf':5,\n","                    'W29317.pdf':7,'W29334.pdf':6,'W30188.pdf':22,'W30189.pdf':4,'W30789.pdf':5,'W36047.pdf':44,'W90244.pdf':11,'W90258.pdf':13,\n","                    'W90329.pdf':7}"],"metadata":{"id":"GhBgef-RstbR","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":304,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 2. Connect to Drive and Install Libraries\n","def connect_drive():\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"metadata":{"id":"B0ONaXtIoQVx","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 3. Extract Information from PDF Files\n","# This function determines the pattern to extract the well name based on the given PDF filename.\n","def well_name_func(pdf):\n","    if pdf.split('/')[-1] == 'W23359.pdf':\n","        well_name_pattern = 'Well Name .+ \\n(.*?)(?:\\n|$)'\n","    elif pdf.split('/')[-1] == 'W28601.pdf':\n","        well_name_pattern = 'Well Name .+ \\n(.*?)(?:\\n|$|Before After)'\n","    elif pdf.split('/')[-1] in ['W20863.pdf','W22731.pdf','W20864.pdf','W20407.pdf']:\n","        well_name_pattern = 'Well or Facility Name : (.*?)(?:\\n|$)'\n","    else:\n","        well_name_pattern = 'Well Name and Number \\n(.*?)(?:\\n|$)'\n","    return well_name_pattern"],"metadata":{"id":"kYVwOHOMo4Ou","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# This function determines the pattern to extract the API number based on the given PDF filename.\n","def api_func(pdf):\n","    if pdf.split('/')[-1] == 'W90258.pdf':\n","        API_pattern = '\\d{2}\\s?-\\s?\\d{3}\\s?-\\s?\\d{5}'\n","    elif pdf.split('/')[-1] == 'W20407.pdf':\n","        API_pattern = '31-19H.*API\\s(.*)'\n","    elif pdf.split('/')[-1] in ['W21796.pdf','W20863.pdf','W21266.pdf']:\n","        API_pattern = '\\d{3}\\s?-?\\s?\\d{5}'\n","    else:\n","        API_pattern = '\\d{2}-\\d{3}-\\d{5}'\n","    return API_pattern"],"metadata":{"id":"41_Hl5DDpKwx","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# This function fixes the API number format based on the given PDF filename and the matched API pattern.\n","def api_fix_func(pdf, API_pattern_match):\n","    if pdf.split('/')[-1] in ['W21796.pdf','W20863.pdf','W21266.pdf']:\n","        api_num = API_pattern_match.group(0)\n","        api_num_fixed = '33-'+api_num.split()[0]+'-'+api_num.split()[1]\n","    elif pdf.split('/')[-1] == 'W20407.pdf':\n","        api_num = API_pattern_match.group(1).strip()\n","        api_num_fixed = api_num[:2]+'-'+api_num[3:6]+'-'+api_num[6:]\n","    elif pdf.split('/')[-1] == 'W90258.pdf':\n","        api_num = API_pattern_match.group(0)\n","        api_num_fixed = api_num.replace(' ',\"\")\n","    else:\n","        api_num_fixed = API_pattern_match.group(0)\n","    return api_num_fixed"],"metadata":{"id":"hSwl6Q3QpL_S","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def extract_well_data_from_pdfs(folder_path):\n","    # Get a list of all PDF files in the given folder\n","    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n","\n","    # Create a dictionary to store extracted data from each PDF\n","    files_dict = defaultdict(dict)\n","\n","    # Iterate over each PDF file\n","    for pdf in pdf_files:\n","        # Open the PDF file for reading\n","        pdf_file = open(pdf, \"rb\")\n","        # Initialize a PDF reader object\n","        pdf_reader = PyPDF2.PdfReader(pdf_file)\n","        # Get the number of pages in the PDF\n","        page_numbers = len(pdf_reader.pages)\n","\n","        # Iterate over each page in the PDF\n","        for page_num in range(page_numbers):\n","            try:\n","                # Get the text content of the page\n","                page = pdf_reader.pages[page_num]\n","                page_text = page.extract_text()\n","\n","                # Get patterns to match well name and API number\n","                well_name_pattern = well_name_func(pdf)\n","                API_pattern = api_func(pdf)\n","\n","                # Try matching the patterns against the page text\n","                well_name_match = re.search(well_name_pattern, page_text)\n","                API_pattern_match = re.search(API_pattern, page_text)\n","\n","                # If a match for well name is found, store it in the dictionary\n","                if well_name_match:\n","                    files_dict[pdf.split('/')[-1]]['Well_name'] = well_name_match.group(1).strip()\n","                    # If both well name and API number are found for a PDF, break out of the loop\n","                    if len(list(files_dict[pdf.split('/')[-1]].keys())) == 2:\n","                        break\n","\n","                # If a match for API number is found, process it further and store in the dictionary\n","                if API_pattern_match:\n","                    api_num_fixed = api_fix_func(pdf, API_pattern_match)\n","                    files_dict[pdf.split('/')[-1]]['API#'] = api_num_fixed.strip()\n","                    # If both well name and API number are found for a PDF, break out of the loop\n","                    if len(list(files_dict[pdf.split('/')[-1]].keys())) == 2:\n","                        print(\"Extract well data from pdfs:\", files_dict[pdf.split('/')[-1]])\n","                        break\n","            except:\n","                # If there's any error in processing a page, move on to the next page\n","                continue\n","\n","            # If we've reached the last page of the PDF and haven't found both pieces of data, print the PDF name\n","            if page_num == page_numbers-1:\n","                print(pdf)\n","\n","\n","\n","    # Return the dictionary with extracted data from all PDFs\n","    return files_dict"],"metadata":{"id":"HcrskIQ3pQDU","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Extracts text from a specific page of a given PDF using OCR.\n","def extract_text_from_pdf(pdf_path, page_number):\n","    # Convert the specified PDF page to an image at 500 DPI\n","    pages = convert_from_path(pdf_path, 500, first_page=page_number, last_page=page_number)\n","    # Use pytesseract to extract text from the image\n","    text = pytesseract.image_to_string(pages[0])\n","    return text"],"metadata":{"id":"CKBZmcq6sxzW","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Extracts specific data from a given string based on patterns and positions.\n","def extract_row1_data(row1_string):\n","\n","    # Remove any \"|\" characters from the string\n","    row1_string = row1_string.replace(\"|\", \"\")\n","\n","    # Split the string into a list of words\n","    row1_list = row1_string.split()\n","\n","    # Initialize default values for extracted data\n","    date_stim, stim_form, top, bottom, stim_stages, volume, vol_units = [None]*7\n","\n","    # Extract data based on conditions and positions\n","    if len(row1_list) >= 7:\n","        if re.match(\"^[a-zA-Z]+$\", row1_list[0]) is None:\n","            date_stim = row1_list[0]\n","        stim_form = row1_list[1]\n","        if (stim_form == 'Three') or (stim_form == '3'):\n","            stim_form = 'Three Forks'\n","        if row1_list[2].isdigit():\n","            top = row1_list[-5]\n","        if row1_list[3].isdigit():\n","            bottom = row1_list[-4]\n","        if row1_list[4].isdigit():\n","            stim_stages = row1_list[-3]\n","        if row1_list[-2].isdigit():\n","            volume = row1_list[-2]\n","        if row1_list[-1] == 'Barrels':\n","            vol_units = row1_list[-1]\n","\n","    return date_stim, stim_form, top, bottom, stim_stages, volume, vol_units"],"metadata":{"id":"Io3_rHwQs26K","executionInfo":{"status":"ok","timestamp":1696720392205,"user_tz":420,"elapsed":2,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Extracts type of treatment from the given string based on specific patterns.\n","def extract_type_treat_data(matched_string):\n","    type_treat = None\n","    if matched_string == 'Sand Frac':\n","        type_treat = matched_string\n","    elif 'Sand Frac' in matched_string:\n","        type_treat = 'Sand Frac'\n","    return type_treat"],"metadata":{"id":"kxtyKTe7s7C-","executionInfo":{"status":"ok","timestamp":1696720392540,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Extracts data from a given row2_string based on specific patterns and positions.\n","def extract_row2_data(row2_string):\n","\n","    # Remove any \"|\" characters from the string\n","    row2_string = row2_string.replace(\"|\", \"\")\n","\n","    # Split the string into a list of words\n","    row2_list = row2_string.split()\n","\n","    # Initialize default values for extracted data\n","    acid, proppant, pressure, rate = [None]*4\n","\n","    # Extract data based on conditions and positions\n","    if len(row2_list) >= 3:\n","        if len(row2_list) == 4:\n","            if row2_list[0].isdigit() and len(row2_list[0]) <= 3:\n","                acid = row2_list[0]\n","            elif row2_list[0].isdigit() and len(row2_list[0]) > 3:\n","                proppant = row2_list[0]\n","            else:\n","                if row2_list[-3].isdigit():\n","                    proppant = row2_list[-3]\n","                pressure = row2_list[-2]\n","                rate = row2_list[-1]\n","        else:\n","            proppant = row2_list[0]\n","            pressure = row2_list[1]\n","            rate = row2_list[2]\n","\n","    return acid, proppant, pressure, rate"],"metadata":{"id":"kaTyFN5rtAYQ","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Extracts details data based on specific conditions.\n","def extract_details_data(details):\n","    # Check specific conditions and return None if any of them are met\n","    if details.startswith('\\nDate') or len(details) >= 500:\n","        return None\n","    return details"],"metadata":{"id":"CM76ulgotCZL","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Extracts stimulation data from a list of PDF files.\n","def extract_stimulation_data_from_pdfs(pdf_files):\n","\n","    stimulation_dict = defaultdict(dict)\n","\n","    # Iterate over each PDF file\n","    for pdf in pdf_files:\n","        pdf_file = open(pdf, \"rb\")\n","        pdf_reader = PyPDF2.PdfReader(pdf_file)\n","\n","        # Retrieve the page number for extraction from a predefined dictionary\n","        page_number = stimulation_page.get(pdf.split('/')[-1], None)\n","\n","        date_stim, stim_form, top, bottom, stim_stages, volume, vol_units, type_treat, acid, proppant, pressure, rate, details= [None]*13\n","\n","        # If a page number is found, extract data from it\n","        if page_number:\n","            text = extract_text_from_pdf(pdf, page_number)\n","\n","            row1_sim_pattern = 'Units\\n(.*)'\n","            type_treat_pattern = 'Min\\)\\n(\\D*)'\n","            row2_sim_pattern = 'Min\\)\\n(.*)'\n","            details_sim_pattern = re.compile(r\"Details\\n(.*?).\\n\\nDa\", re.DOTALL)\n","\n","            row1_sim_pattern_match = re.search(row1_sim_pattern, text)\n","            type_treat_pattern_match = re.search(type_treat_pattern, text)\n","            row2_sim_pattern_match = re.search(row2_sim_pattern, text)\n","            details_sim_pattern_match = re.search(details_sim_pattern, text)\n","\n","            if row1_sim_pattern_match:\n","                row1_string = row1_sim_pattern_match.group(1)\n","                # Extract row1 data from the text\n","                date_stim, stim_form, top, bottom, stim_stages, volume, vol_units = extract_row1_data(row1_string)\n","\n","            if type_treat_pattern_match:\n","                matched_string = type_treat_pattern_match.group(1)\n","                # Extract type of treatment from the text\n","                type_treat = extract_type_treat_data(matched_string)\n","\n","            if row2_sim_pattern_match:\n","                if type_treat is not None:\n","                    row2_string = row2_sim_pattern_match.group(1).replace(type_treat,\"\")\n","                # Extract row2 data from the text\n","                acid, proppant, pressure, rate = extract_row2_data(row2_string)\n","\n","            if details_sim_pattern_match:\n","                # Extract additional details from the text\n","                details = details_sim_pattern_match.group(1)\n","                details = extract_details_data(details)\n","\n","            # Populate the dictionary with extracted data\n","            stimulation_dict[pdf.split('/')[-1]] = {\n","                'Date_stimulated': date_stim,\n","                'Stim_formation': stim_form,\n","                'Top': top,\n","                'Bottom': bottom,\n","                'Stim_stages': stim_stages,\n","                'Volume': volume,\n","                'Units': vol_units,\n","                'Type_treatment': type_treat,\n","                'Acid%': acid,\n","                'Proppant_Lbs': proppant,\n","                'Max_Pressure': pressure,\n","                'Max_Rate': rate,\n","                'Details': details\n","            }\n","\n","            print(\"extract stimulation data from pdfs:\", stimulation_dict[pdf.split('/')[-1]])\n","\n","        pdf_file.close()\n","\n","    return stimulation_dict"],"metadata":{"id":"JYvPqkXbtX2q","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# 4. Web Scraping Information\n","def scrape_well_data(api_df):\n","\n","    # api_df is the dataframe\n","\n","    driver = webdriver.Chrome()\n","    url = 'https://www.drillingedge.com/search'\n","    driver.get(url)\n","    time.sleep(10)\n","    api_input = driver.find_element_by_name('33-053-06028')\n","\n","    web_df = pd.DataFrame()\n","\n","    # for api_number in api_numbers:\n","        # ... [Rest of the code for web scraping]\n","    driver.quit()\n","\n","    return web_df"],"metadata":{"id":"AXvclhUNqQKy","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Merge the data from two dictionaries into a single dataframe and save to a CSV.\n","\n","def merge_and_save_dataframes(files_dict, stimulation_dict, output_folder):\n","    # Convert the dictionaries to dataframes\n","    df = pd.DataFrame.from_dict(files_dict,orient='index')\n","    df_stimulation = pd.DataFrame.from_dict(stimulation_dict,orient='index')\n","\n","    # Merge the two dataframes on their indexes (horizontally)\n","    merged_df = pd.concat([df, df_stimulation], axis=1)\n","    print(\"Information Extracted from PDF: \", merged_df)\n","\n","    # Save the merged dataframe to a CSV file\n","    merged_df.to_csv(output_folder + \"Task_PDF_original.csv\")\n","\n","    return merged_df"],"metadata":{"id":"wFt5_SXuqOLv","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":7,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def preprocess_and_save_dataframes(df, output_folder, task):\n","\n","    if task == \"a\":\n","        # Remove HTML tags\n","        def remove_html_tags(text):\n","            clean = re.compile('<.*?>')\n","            return re.sub(clean, '', str(text))\n","\n","        # Remove special characters (except for alphanumeric characters and spaces)\n","        def remove_special_characters(text):\n","            return re.sub('[^A-Za-z0-9\\s\\./-]+', '', str(text))\n","\n","        processed_df = df.applymap(remove_html_tags)\n","        processed_df = processed_df.applymap(remove_special_characters)\n","\n","        # Handle missing data\n","        # Replace NaN values in numeric columns with 0 and in string columns with \"N/A\"\n","        for column in processed_df.columns:\n","            if processed_df[column].dtype == 'object':  # If column is of object type, it's considered a string\n","                processed_df[column].fillna(\"N/A\", inplace=True)\n","            else:  # If column is numeric\n","                processed_df[column].fillna(0, inplace=True)\n","\n","        processed_df.replace(\"None\", \"nan\", inplace=True)\n","\n","        # Preprocess the \"\\n\"\n","        processed_df.replace(\"\\n\", \";\", regex=True, inplace=True)\n","        processed_df['Details'] = processed_df['Details'].str.replace(r\"(;[\\s;]+)\", \";\", regex=True).str.strip(\";\")\n","\n","        # Save the merged dataframe to a CSV file\n","        processed_df.to_csv(output_folder+'Task1_PDF_preprocessed.csv', index=False)\n","\n","    else:\n","        # Save the merged dataframe to a CSV file\n","        processed_df.to_csv(output_folder+'Task1_Web_preprocessed.csv', index=False)\n","\n","        pass\n","\n","\n","    return processed_df"],"metadata":{"id":"8PX3XPdx_Ojm","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":6,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def store_db(df):\n","\n","  # TODO: Store A to database\n","\n","  pass"],"metadata":{"id":"Zf3IXLbxQ3ZZ","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":6,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def update_db(df):\n","\n","  # TODO: Update B to database\n","\n","  pass"],"metadata":{"id":"S8T_WecxRBGN","executionInfo":{"status":"ok","timestamp":1696720392541,"user_tz":420,"elapsed":6,"user":{"displayName":"Nikki Seina","userId":"08236356394287616531"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Running the code\n","if __name__ == \"__main__\":\n","    # Establish a connection to Google Drive (assuming a relevant function is defined elsewhere)\n","    connect_drive()\n","\n","    # Define the folder path containing the PDF files and retrieve the list of PDF files\n","    folder_path = '/content/drive/Shareddrives/560_GROUP/DSCI560_Lab5'\n","    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n","\n","    # Extract well data and stimulation data from the PDFs\n","    files_dict = extract_well_data_from_pdfs(folder_path)\n","    stimulation_dict = extract_stimulation_data_from_pdfs(pdf_files)\n","\n","    # Define the output folder path\n","    output_folder = '/content/drive/Shareddrives/560_GROUP/Lab5_outputs/'\n","\n","    # Merge the extracted data and save to a CSV\n","    merged_df = merge_and_save_dataframes(files_dict, stimulation_dict, output_folder)\n","\n","    # preprocess the original data\n","    processed_df = preprocess_and_save_dataframes(merged_df, output_folder, \"a\")\n","\n","\n","    # store_db(processed_df)\n","\n","    # merge_df is the unprep dataframe that from a,\n","    api_numbers = merged_df[\"API#\"]\n","    web_df = scrape_well_data(api_numbers)\n","\n","    # processed_web_df = preprocess_and_save_dataframes(web_df, output_folder, \"b\")\n","\n","    # update_db(processed_web_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-ICUmUgqSkA","outputId":"38906cc9-3d57-4c18-e056-3a6d1e8e0aaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/560_GROUP/DSCI560_Lab5/W11920.pdf\n","Extract well data from pdfs: {'Well_name': 'MAGNUM  3-36-25H', 'API#': '33-053-04069'}\n","Extract well data from pdfs: {'Well_name': 'MAGNUM 2-36-25H', 'API#': '33-053-03944'}\n","Extract well data from pdfs: {'Well_name': 'Colville 5301 44-12T', 'API#': '33-053-04981'}\n","Extract well data from pdfs: {'Well_name': 'DAHL FEDERAL 2-15H', 'API#': '33-002-58854'}\n","Extract well data from pdfs: {'Well_name': 'Magnum 1-36-25H', 'API#': '33-053-03943'}\n","Extract well data from pdfs: {'Well_name': '(see details', 'API#': '33-053-06010'}\n","Extract well data from pdfs: {'Well_name': 'DAHL 15-11H', 'API#': '33-002-58854'}\n","Extract well data from pdfs: {'Well_name': 'BRAY  5301 43-12H', 'API#': '33-053-03911'}\n","Extract well data from pdfs: {'Well_name': 'FOLEY FEDERAL  5301 43-12H', 'API#': '33-000-10000'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers Wade Federal 5300 44-24 12TXR', 'API#': '33-053-06012'}\n","Extract well data from pdfs: {'Well_name': 'Cc lumbus Federal 1-16H', 'API#': '33-053-04852'}\n","Extract well data from pdfs: {'Well_name': 'Gramma Federal 5300 41-3113T2', 'API#': '33-053-06232'}\n","/content/drive/Shareddrives/560_GROUP/DSCI560_Lab5/W15358.pdf\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 31-18 6B', 'API#': '33-053-06057'}\n","Extract well data from pdfs: {'Well_name': '(see details', 'API#': '33-053-06011'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 4-6H', 'API#': '33-105-02729'}\n","Extract well data from pdfs: {'Well_name': 'Columbus Federal 3-16H', 'API#': '33-053-04856'}\n","Extract well data from pdfs: {'Well_name': 'Columbus Federal 2-16H', 'API#': '33-053-04855'}\n","Extract well data from pdfs: {'Well_name': '(see details', 'API#': '33-053-05924'}\n","Extract well data from pdfs: {'Well_name': 'lnnoko 5301 43-12T', 'API#': '33-053-03911'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 2-6H I Approximate Start Date D Drilling Prognosis', 'API#': '33-105-02731'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta Federal 8-6H', 'API#': '33-105-02725'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta Federal 10-6H', 'API#': '33-105-02723'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta #1 SWD I Approximate Start Date D Drilling Prognosis', 'API#': '33-105-90258'}\n","Extract well data from pdfs: {'Well_name': 'Carson SWD 5301 12-24', 'API#': '33-053-90329'}\n","Extract well data from pdfs: {'Well_name': 'Gramma Federal 5300 41-31 12B', 'API#': '33-053-06231'}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /90ms-RKSJ-H not implemented yet\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /90ms-RKSJ-V not implemented yet\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Extract well data from pdfs: {'Well_name': 'Atlanta 13-6H', 'API#': '33-105-02720'}\n","Extract well data from pdfs: {'Well_name': 'AUmta 12-6H Sec 5, 6, 7, & 8 T153N R101W', 'API#': '33-105-02721'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta Federal 7-6H', 'API#': '33-105-02726'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta Federal 9-6H', 'API#': '33-105-02724'}\n","Extract well data from pdfs: {'Well_name': 'Tallahassee 3-16H', 'API#': '33-053-04853'}\n","Extract well data from pdfs: {'Well_name': 'BUCK SHOT SWD 5300 31-31', 'API#': '33-053-90244'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta Federal 5-6H', 'API#': '33-105-02728'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 1-6H', 'API#': '33-105-02732'}\n","Extract well data from pdfs: {'Well_name': 'Wade Federal 5300 21-30 12T', 'API#': '33-053-06129'}\n","Extract well data from pdfs: {'Well_name': 'Wade Federal 5300 41-30 4T -+ Se~ t>c.~o...~ls', 'API#': '33-053-06051'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 6300 41-1814BX D', 'API#': '33-053-06029'}\n","Extract well data from pdfs: {'Well_name': 'WADE FEDERAL 5300 41-30 4T', 'API#': '33-053-05943'}\n","Extract well data from pdfs: {'Well_name': 'CHALMERS 5300 21-1910T', 'API#': '33-053-06022'}\n","Extract well data from pdfs: {'Well_name': 'Kline FederaUi300 11-18 3T ....... \".• Footages · .', 'API#': '33-053-06225'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers 5300 21-19 BT', 'API#': '33-053-06021'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers 5300 21-19 68', 'API#': '33-053-06019'}\n","Extract well data from pdfs: {'Well_name': 'WADE FEDERAL 5300 21-30 13B', 'API#': '33-053-06131'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 3-6H', 'API#': '33-105-02730'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers 5300 21-19 11B 24-HOUR PRODUCTION RATE', 'API#': '33-053-06024'}\n","Extract well data from pdfs: {'Well_name': '(see details) 24-HOUR PRODUCTION RATE', 'API#': '33-053-05995'}\n","Extract well data from pdfs: {'Well_name': 'WADE FEDERAL 5300 41-30 GB', 'API#': '33-053-05954'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers 5300 21-19 98', 'API#': '33-053-06023'}\n","Extract well data from pdfs: {'Well_name': 'Wade Federal 5300 31-30 11T', 'API#': '33-053-05906'}\n","Extract well data from pdfs: {'Well_name': 'WADE FEDERAL 5300 31-30 2B', 'API#': '33-053-05995'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 11-18 58 ...', 'API#': '33-053-06223'}\n","Extract well data from pdfs: {'Well_name': 'Tallahassee 2-16H', 'API#': '33-053-04854'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 41-18 BT', 'API#': '33-053-06025'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 31-18 7T', 'API#': '33-053-06056'}\n","Extract well data from pdfs: {'Well_name': 'Chalmers 5300 21-19 ST', 'API#': '33-053-06018'}\n","Extract well data from pdfs: {'Well_name': 'Kllne Federal 5300 11-18 28 l D', 'API#': '33-053-06243'}\n","Extract well data from pdfs: {'Well_name': 'Kllne Federal 6300 41-18 13T2X D', 'API#': '33-053-06028'}\n","Extract well data from pdfs: {'Well_name': 'Ash Federal 5300 11-18T', 'API#': '33-053-04211'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 41-18 12TX', 'API#': '33-053-06030'}\n","Extract well data from pdfs: {'Well_name': 'KLINE FEDERAL 5300 31-18 15T', 'API#': '33-053-06755'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 11-6H', 'API#': '33-105-02722'}\n","Extract well data from pdfs: {'Well_name': 'Kline Federal 5300 31-18 8B', 'API#': '33-053-00605'}\n","Extract well data from pdfs: {'Well_name': 'Atlanta 14-SH', 'API#': '33-105-02719'}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': None, 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '3630945', 'Max_Pressure': '7548', 'Max_Rate': '34.0', 'Details': 'Fractured the Middle Bakken with 35, stages using fracturing sleeves and packers, with 320087# of 100 Mesh Sand, 3310858# of 20/40 White Sand,\\nand 40332 bbls of clean water'}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': None, 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '1160090', 'Max_Pressure': '8403', 'Max_Rate': '39.7', 'Details': '\\nVolume Units '}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': None, 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': None, 'Acid%': None, 'Proppant_Lbs': '1160090', 'Max_Pressure': '8403', 'Max_Rate': '39.7', 'Details': None}\n","extract stimulation data from pdfs: {'Date_stimulated': '05/19/2012', 'Stim_formation': 'Bakken', 'Top': '10956', 'Bottom': '20924', 'Stim_stages': '20', 'Volume': '77312', 'Units': 'Barrels', 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '3053400', 'Max_Pressure': '§3844', 'Max_Rate': '57.0', 'Details': 'Pumped 1,992,000 20/40 sand, 97,400# 30/70 sand, 964,000# 40/70 sand, 77,312 Bbis total fluid in 20 stages over 3 days (5/17/12 - 5/19/12) at a\\nmaximum rate of 57.0 bpm and maximum pressure of 8844 psi'}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': None, 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': None, 'Acid%': None, 'Proppant_Lbs': '3053400', 'Max_Pressure': '§3844', 'Max_Rate': '57.0', 'Details': '\\nType Treatment\\n\\nDetai'}\n","extract stimulation data from pdfs: {'Date_stimulated': '10/30/2014', 'Stim_formation': 'Bakken', 'Top': '10910', 'Bottom': '20833', 'Stim_stages': '36', 'Volume': '206015', 'Units': 'Barrels', 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '4076819', 'Max_Pressure': '9177', 'Max_Rate': '75.0', 'Details': '40/70 Ceramic: 1513408\\n30/50 Ceramic: 2259661\\n100 mesh: 30375'}\n","extract stimulation data from pdfs: {'Date_stimulated': '01/11/2012', 'Stim_formation': 'Bakken', 'Top': '10952', 'Bottom': '20850', 'Stim_stages': '13', 'Volume': '1822380', 'Units': None, 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '1450600', 'Max_Pressure': '8842', 'Max_Rate': '45.5', 'Details': 'Pumped 999,800# 20/40 sand, 450,800 40/70 sand, 1,822,380 Gals total fluid in 13 stages over 4 days (1/8/11 - 1/11/12) at a maximum rate of 45.5\\nbpm and maximum pressure of 8842 psi'}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': None, 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': None, 'Acid%': None, 'Proppant_Lbs': '1450600', 'Max_Pressure': '8842', 'Max_Rate': '45.5', 'Details': '40/70 Sand- 1,824,600\\n20/40 Ceramic-2,553,22'}\n","extract stimulation data from pdfs: {'Date_stimulated': None, 'Stim_formation': 'Treatment', 'Top': None, 'Bottom': None, 'Stim_stages': None, 'Volume': None, 'Units': None, 'Type_treatment': 'Sand Frac', 'Acid%': None, 'Proppant_Lbs': '4519292', 'Max_Pressure': '8877', 'Max_Rate': '43.6', 'Details': '40/70 white- 1825104\\n20/40 white- 269418'}\n"]}]},{"cell_type":"code","source":["    # processed_df is the preprocessd dataframe from task a. You can check this format.\n","    api_df = processed_df[[\"Well_name\", \"API#\"]]\n","\n","    api_df\n","    # # test your function here\n","    # web_df = scrape_well_data(api_df)\n","    # web_df"],"metadata":{"id":"yYPSeZhZWVno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.common.keys import Keys\n","import chromedriver_autoinstaller\n","import time\n","import pandas as pd\n","\n","chromedriver_autoinstaller.install()\n","\n","# Initialize the Chrome WebDriver with options\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n","chrome_options.add_argument('--no-sandbox')  # Disable sandboxing for Colab\n","chrome_options.add_argument('--disable-dev-shm-usage')  # Disable shared memory usage\n","\n","# Initialize the Chrome WebDriver\n","driver = webdriver.Chrome(options=chrome_options)"],"metadata":{"id":"Fv1yb5WjY4a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for index, row in api_df:\n","\n","  well_name = row['Well_name']\n","  api_num = row['API#']\n","\n","  well_name_url = \"-\".join(well_name.replace('-',\" \").split()).lower()\n","\n","  url = 'https://www.drillingedge.com/north-dakota/mckenzie-county/wells/' + well_name_url + \"/\"+api_num\n","\n","  driver.get(url)\n","  time.sleep(5)\n","\n","  # Get the page source after waiting\n","  page_source = driver.page_source\n","\n","  # Parse the page source with BeautifulSoup\n","  soup = BeautifulSoup(page_source, 'html.parser')\n","\n","  # Locate the table with the well information\n","  well_info_table = soup.find('table', {'class': 'skinny'})\n","\n","  well_status = []\n","  well_type = []\n","  closest_city = []\n","  # Find all rows in the table\n","  rows = well_info_table.find_all('tr')\n","\n","  for row in rows:\n","      header = row.find_all('th')\n","      data = row.find_all('td')\n","      for h, d in zip(header, data):\n","          header_text = h.text.strip()\n","          data_text = d.text.strip()\n","          if header_text == 'Well Status':\n","              well_status.append(data_text)\n","          if header_text == 'Well Type':\n","              well_type.append(data_text)\n","          if header_text == 'Closest City':\n","              closest_city.append(data_text)\n","\n","\n","  print(well_status)\n","  print(well_type)\n","  print(closest_city)\n","\n","\n","# Close the WebDriver when finished\n","driver.quit()"],"metadata":{"id":"NZwZTWsiYnEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K3OFvY2maZXo"},"execution_count":null,"outputs":[]}]}