# DSCI560 - Data Science Professional Practicum

Team: N.A.H


## Lab3: Stock Portfolio Management System

A simple application that lets users create and manage their stock portfolios using data fetched from the Yahoo Finance API. Users can store their custom portfolios in a MySQL database for later retrieval.

Please go to the lab3 directory to see the full README file: [Lab3 README](lab3/readme.md)


## Lab3-part2: Stock Portfolio Management System (Cont.)

This project implements a trading algorithm using four different forecasting methods: 
1. Moving Average
2. Exponential Smoothing
3. ARIMA (AutoRegressive Integrated Moving Average)
4. LSTM (Long Short-Term Memory networks)

The algorithm fetches stock data for selected tickers, forecasts the stock prices using each method, then trades based on the LSTM forecast. Performance metrics are also calculated for each model and the overall trading strategy.

Please go to the lab3-part2 directory to see the full README file: [Lab3-part2 README](lab3-part2/README.md)

## Lab4-part1: Running the Reddit Post Scraper with Batch Processing

This program allows you to scrape new posts from a specified subreddit using the PRAW (Python Reddit API Wrapper). This script employs batch processing to efficiently fetch the desired number of posts. Once fetched, the posts are processed to extract relevant information and store it in a MySQL database.

Please go to the lab4-part1 directory to see the full README file: [Lab4-part1 README](lab4-part1/README.pdf)

## Lab4-part2: Running the Reddit Post Scraper with Batch Processing (Cont.)

This script is designed to scrape posts from a specific Reddit subreddit using the PRAW library. It further processes the posts, creates embeddings using the Doc2Vec algorithm, and then performs clustering using the KMeans algorithm. The results are then stored in a MySQL database.

Please go to the lab4-part2 directory to see the full README file: [Lab4-part2 README](lab4-part2/readme.pdf)

## Lab5-part1: Extract Web Well-data

The provided code performs the following tasks:
1. Extracts well data and stimulation data from a list of PDF files.
2. Uses web scraping to gather additional information about each well based on its well name.
3. Preprocesses the extracted and scraped data.
4. Stores the final data in a MySQL database.

please go to the lab5-part1 directory to see the full README file: [Lab5-part1 README](lab5-part1/README.md)

## Lab5-part2: Extract Web Well-data (Cont.)

This project will walk you through the setup and execution of your web project. We will begin by setting up the Apache web server, followed by the installation of Node.js and other required tools. Lastly, we will initialize and run the provided application.

Please go to the lab5-part2 directory to see the full README file: [Lab5-part2 README](lab5-part2/Readme.pdf)

## Lab6-part1: PDFs Chatbot using Langchain, GPT 3.5
This is a Python gui application that demonstrates how to build a custom PDF chatbot using LangChain and GPT 3.5.

Please go to the lab6-part1 directory to see the full README file: [Lab6-part1 README](lab6-part1/README.pdf)

## Lab-part2: PDFs Masters - Chat with your PDFs!

The application provides a chat interface, allowing users to ask questions about the content of their uploaded PDFs. With the help of various libraries, the app extracts text from PDFs, chunks the data, and utilizes embeddings and vector stores to support a conversational experience with the content.

Please go to the lab6-part2 directory to see the full README file: [Lab6-part2 README](lab6-part2/readme.md)